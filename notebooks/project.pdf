{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdc236f3-1f44-4c65-bac3-7fe7e067120f",
   "metadata": {},
   "source": [
    "# IDS classification using NSL-KDD dataset\n",
    "\n",
    "Released by Th√©ophane Dumas and Cyrille Tryer\n",
    "\n",
    "### Goal\n",
    "\n",
    "The goal of this project was to compare two classification models that can be used by an Intrusion Detection System.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use then well known NSL-KDD model. \n",
    "\n",
    "### References\n",
    "\n",
    "This report is inspired from this work : [https://github.com/thinline72/nsl-kdd]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e59356-106d-4cca-964b-8221a3c1a4a4",
   "metadata": {},
   "source": [
    "## 1. Data processing\n",
    "\n",
    "First we connect to Spark, and we load the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc54638-b297-4738-a42b-abb6fe2c75e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, udf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, split, col\n",
    "import pyspark.sql.functions as sql\n",
    "\n",
    "context = SparkSession.builder.appName('projet').getOrCreate().sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "736ce200-fa9d-44ab-93fc-6c695b69b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load dataset and divide it into 8 partitions\n",
    "col_names = np.array([\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n",
    "    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n",
    "    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n",
    "    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n",
    "    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n",
    "    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n",
    "    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n",
    "    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n",
    "    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"labels\"])\n",
    "\n",
    "def load_dataset(path):\n",
    "    dataset_rdd = context.textFile(path, 8).map(lambda line: line.split(','))\n",
    "    dataset_df = (dataset_rdd.toDF(col_names.tolist()).select(\n",
    "                    col('duration').cast(DoubleType()),\n",
    "                    col('protocol_type').cast(StringType()),\n",
    "                    col('service').cast(StringType()),\n",
    "                    col('flag').cast(StringType()),\n",
    "                    col('src_bytes').cast(DoubleType()),\n",
    "                    col('dst_bytes').cast(DoubleType()),\n",
    "                    col('land').cast(DoubleType()),\n",
    "                    col('wrong_fragment').cast(DoubleType()),\n",
    "                    col('urgent').cast(DoubleType()),\n",
    "                    col('hot').cast(DoubleType()),\n",
    "                    col('num_failed_logins').cast(DoubleType()),\n",
    "                    col('logged_in').cast(DoubleType()),\n",
    "                    col('num_compromised').cast(DoubleType()),\n",
    "                    col('root_shell').cast(DoubleType()),\n",
    "                    col('su_attempted').cast(DoubleType()),\n",
    "                    col('num_root').cast(DoubleType()),\n",
    "                    col('num_file_creations').cast(DoubleType()),\n",
    "                    col('num_shells').cast(DoubleType()),\n",
    "                    col('num_access_files').cast(DoubleType()),\n",
    "                    col('num_outbound_cmds').cast(DoubleType()),\n",
    "                    col('is_host_login').cast(DoubleType()),\n",
    "                    col('is_guest_login').cast(DoubleType()),\n",
    "                    col('count').cast(DoubleType()),\n",
    "                    col('srv_count').cast(DoubleType()),\n",
    "                    col('serror_rate').cast(DoubleType()),\n",
    "                    col('srv_serror_rate').cast(DoubleType()),\n",
    "                    col('rerror_rate').cast(DoubleType()),\n",
    "                    col('srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('same_srv_rate').cast(DoubleType()),\n",
    "                    col('diff_srv_rate').cast(DoubleType()),\n",
    "                    col('srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_count').cast(DoubleType()),\n",
    "                    col('dst_host_srv_count').cast(DoubleType()),\n",
    "                    col('dst_host_same_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_diff_srv_rate').cast(DoubleType()),\n",
    "                    col('dst_host_same_src_port_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_diff_host_rate').cast(DoubleType()),\n",
    "                    col('dst_host_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_serror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_rerror_rate').cast(DoubleType()),\n",
    "                    col('dst_host_srv_rerror_rate').cast(DoubleType()),\n",
    "                    col('labels').cast(StringType())))\n",
    "\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5a3efa-1e4d-48a7-bd64-c6f936bfd82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset('kdd/KDDTest+.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97016ea-1201-436d-81df-1d5a8577de06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of lines loaded : 22544\n"
     ]
    }
   ],
   "source": [
    "print('number of lines loaded :', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f884aad-801d-4b16-9f82-7bad64105d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Top 10 labels :\n",
      "+------------+-----+\n",
      "|      labels|count|\n",
      "+------------+-----+\n",
      "|      normal| 9711|\n",
      "|     neptune| 4657|\n",
      "|guess_passwd| 1231|\n",
      "|       mscan|  996|\n",
      "| warezmaster|  944|\n",
      "|     apache2|  737|\n",
      "|       satan|  735|\n",
      "|processtable|  685|\n",
      "|       smurf|  665|\n",
      "|        back|  359|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('# Top 10 labels :')\n",
    "df.groupBy('labels').count().sort('count', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0040e5-32ec-4197-91c0-f76971c7a1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Top 10 services :\n",
      "+--------+-----+\n",
      "| service|count|\n",
      "+--------+-----+\n",
      "|    http| 7853|\n",
      "| private| 4774|\n",
      "|  telnet| 1626|\n",
      "|   pop_3| 1019|\n",
      "|    smtp|  934|\n",
      "|domain_u|  894|\n",
      "|ftp_data|  851|\n",
      "|   other|  838|\n",
      "|   ecr_i|  752|\n",
      "|     ftp|  692|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('# Top 10 services :')\n",
    "df.groupBy('service').count().sort('count', ascending=False).limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5821e549-6f25-4ad4-a721-b254b98d8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we separate training data from test\n",
    "train, test = df.randomSplit([0.7, 0.3], seed = 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dabb89-9e90-469b-843b-f2e1d764abcd",
   "metadata": {},
   "source": [
    "## 2. Prediction with NaiveBayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "547973af-f24c-40b6-a335-f3e9c0c716d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# String indexer\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = []\n",
    "\n",
    "for col in test.schema :\n",
    "    if str(col.dataType) == 'StringType':\n",
    "        try:\n",
    "            si = StringIndexer(inputCol = col.name, outputCol = col.name + '_idx')\n",
    "            stages += [si]\n",
    "            # df = indexer.fit(df).transform(df)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "assembler = VectorAssembler(outputCol=\"features\")\n",
    "feature_names = [col.name for col in df.schema if col.name not in [\"labels\", \"protocol_type\", \"service\", \"flag\"]]\n",
    "assembler.setInputCols(feature_names)\n",
    "stages += [assembler]\n",
    " \n",
    "# assembler = VectorAssembler(outputCol=\"label\")\n",
    "# assembler.setInputCols([\"labels_idx\"])\n",
    "# stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6204bac7-ca76-463f-b75c-7d7f8d9afee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_data = pipeline.fit(test).transform(train)\n",
    "train_data = data.withColumnRenamed(\"labels_idx\", \"label\")\n",
    "\n",
    "test_data = pipeline.fit(test).transform(test)\n",
    "test_data = data.withColumnRenamed(\"labels_idx\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8ae6c37-80ee-4052-ba5a-9dd175848938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naives Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d5d2236a-8ecc-4434-8ef2-5999ae3f5cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m34\u001b[39m]\n\u001b[1;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MultilayerPerceptronClassifier(maxIter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, layers\u001b[38;5;241m=\u001b[39mlayers, blockSize\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1234\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    164\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[0;32m--> 335\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;124;03m    fitted Java model\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Multilayer-perceptron\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "\n",
    "layers = [4, 5, 4, 34]\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "model = trainer.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5f29e-b061-4fb1-b93b-0d7c785d9442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial logistic regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fee994b-c629-4bef-8486-b56a5b4d3f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 predictions : \n",
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "| 14.0|      14.0|[7.56878387153422...|\n",
      "| 14.0|      14.0|[9.82259348467383...|\n",
      "| 14.0|      14.0|[1.86829813995521...|\n",
      "| 14.0|      14.0|[6.55365819993266...|\n",
      "| 14.0|      14.0|[9.71541916872345...|\n",
      "| 14.0|      14.0|[1.47099760879319...|\n",
      "| 14.0|      14.0|[2.20381967722288...|\n",
      "| 11.0|      14.0|[1.25925461026678...|\n",
      "| 11.0|      14.0|[4.17618010391384...|\n",
      "| 11.0|      14.0|[5.00109131028138...|\n",
      "+-----+----------+--------------------+\n",
      "\n",
      "accuracy in test dataset : 0.33222474608917524\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print('First 10 predictions : ')\n",
    "predictions.limit(10).select(\"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "print('accuracy in test dataset :', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
